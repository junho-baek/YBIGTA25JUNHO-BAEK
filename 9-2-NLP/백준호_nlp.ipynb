{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQx1Dj7XbS-i"
      },
      "source": [
        "# **9. NLP 세션 과제**\n",
        "\n",
        "- 이번 과제는 BPE 토크나이저를 직접 구현해보고, 훈련시켜보는 것입니다.\n",
        "\n",
        "- **각 셀의 실행 결과물을 같이** 저장해서, 드라이브에 업로드 된 과제 명세에 적혀있는 제출 방식을 참고하여 본 노트북 파일을 제출해주세요. </br></br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **주의사항!**\n",
        "\n",
        "- BPE를 구현하는 코드는 조금만 검색해도 쉽게 찾을 수 있습니다. (유명하니까요...) 하지만 해당 코드를 먼저 찾아보는 것은 추천하지 않습니다.\n",
        "\n",
        "- 본 과제의 목적은 구현 능력을 기르는 데 있습니다. 이론을 실제 코드로 구현하는 능력이 있고, 없고는 굉장히 큰 차이입니다!!\n",
        "\n",
        "- 따라서, 본 과제를 수행할 때, **BPE의 이론을 먼저\n",
        "충분히 복습**하고, **다른 샘플 코드를 보지 않고** 각각의 기능을 어떻게 구현할지 충분히 고민해보시기 바랍니다!\n",
        "\n",
        "- 인터넷에 샘플 코드가 많다는 건 여러분들도 충분히 하실 수 있다는 의미니까요!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUAzagMqbZad"
      },
      "source": [
        "## 1. BPE Tokenizer 구현하기 (70점, 각 세부 파트별 부분점수 있음)\n",
        "\n",
        "  **BPE Tokenizer**를 구현하기 위해서, 크게 다음 **세가지** 부분으로 쪼갤 수 있습니다.\n",
        "\n",
        "```\n",
        "  1) 전체 corpus를 공백으로 분리하고, {단어:빈도수} 형태의 vocabulary 사전을 구성하는 부분\n",
        "\n",
        "  2) 구성된 vocabulary 사전을 순회하면서, 사전 내 character 토큰 및 각각의 등장 횟수를 반환해 주는 부분\n",
        "\n",
        "  3) text에서 가장 자주 등장한 pair을 연결해주는 부분\n",
        "```\n",
        "\n",
        "본 과제에서 하실 일은 각각의 함수 코드를 직접 작성해보고, 각각의 함수를 모두 합쳐 최종적으로 BPE Tokenizer를 구현하는 것입니다!\n",
        "\n",
        "**Hint**: Collections 패키지의 defaultdict 클래스의 활용법을 찾아보시면 좀 더 손쉽게 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P8viuw3utO0f"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzvipA3QqYth"
      },
      "source": [
        "여러분이 작성한 코드가 맞는지 스스로 감을 잡으실 수 있도록, 각 함수 별로 샘플 corpus를 함수에 넣었을 때 나와야 하는 결과를 함께 제시하도록 하겠습니다. 샘플 corpus는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fAVFREwObZAR"
      },
      "outputs": [],
      "source": [
        "sample_corpus =  [ 'YBAGTA YBECTA YBIGTA YBOGTA YBUGTA',\n",
        "         'I love YBIGTA',\n",
        "          'I like YBIGTA',\n",
        "          'what is YBAGTA']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJaDz00rq8y5"
      },
      "source": [
        "### 1) 사전만들기 함수 (20점)\n",
        "\n",
        " 첫번째 함수는 corpus가 입력으로 주어졌을 때, 이에 기반하여 Vocabulary 사전을 구축하는 함수입니다. 이때, input/output 형태는 다음과 같습니다.\n",
        "\n",
        "\n",
        "  - `input` : 여러 개의 문자열로 이루어진 리스트 (sample corpus와 형태 동일)\n",
        "  - `output` : 여러 개의 단어:빈도수 쌍으로 이루어진 딕셔너리 (예시: `{단어1:빈도수1, 단어2:빈도수2, ... }` )\n",
        "\n",
        "\n",
        "\n",
        "**TODO**:\n",
        "  input 리스트 안의 문자열들에 대해서, 공백을 기준으로 분리한 각각의 단어와 그 빈도수를 딕셔너리 형태로 리턴하는 함수를 정의하세요.\n",
        "\n",
        " 이때, 추후 두번째, 세번째 함수 기능 구현의 편의성을 위해, output 딕셔너리 내 단어 형식을 다음과 같이 맞춰주세요,\n",
        "- 예시: `'Y B I G T A </w>'` -> 글자 사이 공백 추가, 마지막에 postfix 토큰  `</w>` 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UxI-cMFMst4H"
      },
      "outputs": [],
      "source": [
        "def 사전만들기(corpus):\n",
        "\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  vocab = {}\n",
        "\n",
        "  for sentence in corpus:\n",
        "      words = sentence.split()\n",
        "      for word in words:\n",
        "          # 글자 사이에 공백을 추가하고, 마지막에 </w> 추가\n",
        "          formatted_word = ' '.join(list(word)) + ' </w>'\n",
        "          if formatted_word in vocab:\n",
        "              vocab[formatted_word] += 1\n",
        "          else:\n",
        "              vocab[formatted_word] = 1\n",
        "\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvpUlmehueuZ"
      },
      "source": [
        "위에서 정의한 `사전만들기` 함수를 sample_corpus에 적용시 다음과 같은 결과가 리턴되어야 합니다.\n",
        "\n",
        "\n",
        "- 입력:\n",
        "```\n",
        "vocab = 사전만들기(sample_corpus)\n",
        "vocab\n",
        "```\n",
        "\n",
        "- 결과:\n",
        "```\n",
        "{'Y B A G T A </w>': 2,\n",
        " 'Y B E C T A </w>': 1,\n",
        " 'Y B I G T A </w>': 3,\n",
        " 'Y B O G T A </w>': 1,\n",
        " 'Y B U G T A </w>': 1,\n",
        " 'I </w>': 2,\n",
        " 'l o v e </w>': 1,\n",
        " 'l i k e </w>': 1,\n",
        " 'w h a t </w>': 1,\n",
        " 'i s </w>': 1}\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFF9e-zAtCpk",
        "outputId": "862d5287-d978-4062-945a-e7e519965112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Y B A G T A </w>': 2,\n",
              " 'Y B E C T A </w>': 1,\n",
              " 'Y B I G T A </w>': 3,\n",
              " 'Y B O G T A </w>': 1,\n",
              " 'Y B U G T A </w>': 1,\n",
              " 'I </w>': 2,\n",
              " 'l o v e </w>': 1,\n",
              " 'l i k e </w>': 1,\n",
              " 'w h a t </w>': 1,\n",
              " 'i s </w>': 1}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "vocab = 사전만들기(sample_corpus)\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnTUlA3ywksi"
      },
      "source": [
        "### 2) 토큰등장횟수 함수 (20점)\n",
        "\n",
        "다음으로는, `사전만들기`함수로 얻은 vocab 딕셔너리를 입력을 받아서, 사전에 포함된 글자 각각의 등장 횟수를 반환해주는 함수를 구현해 봅시다.\n",
        "\n",
        "인풋, 아웃풋 형태는 다음과 같습니다.\n",
        "\n",
        "- `input` : `사전만들기`함수의 output\n",
        "- `output`: {글자:빈도수} 구조의 tokens 딕셔너리,\n",
        "    예) {'Y':4, 'B':8, .... }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sB1gtxnBtGyA"
      },
      "outputs": [],
      "source": [
        "def 토큰등장횟수(vocab):\n",
        "\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  result = defaultdict(int)\n",
        "    \n",
        "  for word, freq in vocab.items():\n",
        "      tokens = word.split()\n",
        "      for token in tokens:\n",
        "          result[token] += freq\n",
        "  result = dict(result)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gYNuT91mAwb"
      },
      "source": [
        "첫번째로 정의한 `사전만들기`함수에서 반환된 vocab을 `토큰등장횟수`에 넣었을때 반환되는 결과는 다음과 같아야 합니다.\n",
        "\n",
        "```\n",
        "{'Y': 8,\n",
        " 'B': 8,\n",
        " 'A': 10,\n",
        " 'G': 7,\n",
        " 'T': 8,\n",
        " '</w>': 14,\n",
        " 'E': 1,\n",
        " 'C': 1,\n",
        " 'I': 5,\n",
        " 'O': 1,\n",
        " 'U': 1,\n",
        " 'l': 2,\n",
        " 'o': 1,\n",
        " 'v': 1,\n",
        " 'e': 2,\n",
        " 'i': 2,\n",
        " 'k': 1,\n",
        " 'w': 1,\n",
        " 'h': 1,\n",
        " 'a': 1,\n",
        " 't': 1,\n",
        " 's': 1}\n",
        " ```\n",
        "\n",
        " 위와 같은 결과가 나오는지, 다음 셀을 실행하여 확인해볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qalAYLn4z9to",
        "outputId": "d5dbcac2-1d6c-4cee-b59c-0a780c22d8c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'YB': 8,\n",
              " 'A': 10,\n",
              " 'G': 7,\n",
              " 'T': 8,\n",
              " '</w>': 14,\n",
              " 'E': 1,\n",
              " 'C': 1,\n",
              " 'I': 5,\n",
              " 'O': 1,\n",
              " 'U': 1,\n",
              " 'l': 2,\n",
              " 'o': 1,\n",
              " 'v': 1,\n",
              " 'e': 2,\n",
              " 'i': 2,\n",
              " 'k': 1,\n",
              " 'w': 1,\n",
              " 'h': 1,\n",
              " 'a': 1,\n",
              " 't': 1,\n",
              " 's': 1}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "result = 토큰등장횟수(vocab)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O9lm0cK0wn2"
      },
      "source": [
        "### 3) 페어합치기 함수 (30점)\n",
        "\n",
        "마지막 함수입니다. 마지막으로, corpus에서 가장 자주 등장한 페어를 엮어 새롭게 vocab에 추가하는 기능을 구현해봅시다.\n",
        "\n",
        "이를 구현하기 위해서, 다시 다음 두 단계로 쪼갤 수 있습니다.\n",
        "\n",
        "1) `페어등장횟수` (15점): 각 페어가 등장한 횟수를 세어 딕셔너리 형태로 반환\n",
        "- `input` : vocab\n",
        "- `output`: {(글자1,글자2):등장횟수, (글자2,글자3):등장횟수,...} 형식의 딕셔너리\n",
        " -> 아래 샘플 결과 형태를 확인하세요.\n",
        "\n",
        "\n",
        "2) `페어합치기` (15점):\n",
        "-   `input`: vocab, 가장 자주 등장한 Tuple 형태의 pair\n",
        "- `output`: 새롭게 업데이트한 tokens 사전\n",
        "\n",
        "\n",
        "먼저, `페어등장횟수`함수부터 구현해 봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1XCrMW9U0BdX"
      },
      "outputs": [],
      "source": [
        "def 페어등장횟수(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    ##### TODO #####\n",
        "    # 함수 안을 채워주세요 #\n",
        "    # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i+1])] += freq\n",
        "    pairs = dict(pairs)\n",
        "\n",
        "\n",
        "    \n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVtbBVjarbJv"
      },
      "source": [
        "위 함수에 우리의 예시를 넣어보면 다음과 같은 결과가 나와야 합니다.\n",
        "\n",
        "```\n",
        "{('Y', 'B'): 8,\n",
        " ('B', 'A'): 2,\n",
        " ('A', 'G'): 2,\n",
        " ('G', 'T'): 7,\n",
        " ('T', 'A'): 8,\n",
        " ('A', '</w>'): 8,\n",
        " ('B', 'E'): 1,\n",
        " ('E', 'C'): 1,\n",
        " ('C', 'T'): 1,\n",
        " ('B', 'I'): 3,\n",
        " ('I', 'G'): 3,\n",
        " ('B', 'O'): 1,\n",
        " ('O', 'G'): 1,\n",
        " ('B', 'U'): 1,\n",
        " ('U', 'G'): 1,\n",
        " ('I', '</w>'): 2,\n",
        " ('l', 'o'): 1,\n",
        " ('o', 'v'): 1,\n",
        " ('v', 'e'): 1,\n",
        " ('e', '</w>'): 2,\n",
        " ('l', 'i'): 1,\n",
        " ('i', 'k'): 1,\n",
        " ('k', 'e'): 1,\n",
        " ('w', 'h'): 1,\n",
        " ('h', 'a'): 1,\n",
        " ('a', 't'): 1,\n",
        " ('t', '</w>'): 1,\n",
        " ('i', 's'): 1,\n",
        " ('s', '</w>'): 1}\n",
        " ```\n",
        "\n",
        " 위와 같이, 튜플로 묶인 두 개의 토큰 페어와, 각 페어별 등장횟수로 이루어진 딕셔너리가 반환되어야 합니다. 아래 코드를 실행해 결과값이 같은지 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W49OoJPX1NGv",
        "outputId": "8343186c-ff7f-4c17-b110-2c708c061270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('YB', 'A'): 2,\n",
              " ('A', 'G'): 2,\n",
              " ('G', 'T'): 7,\n",
              " ('T', 'A'): 8,\n",
              " ('A', '</w>'): 8,\n",
              " ('YB', 'E'): 1,\n",
              " ('E', 'C'): 1,\n",
              " ('C', 'T'): 1,\n",
              " ('YB', 'I'): 3,\n",
              " ('I', 'G'): 3,\n",
              " ('YB', 'O'): 1,\n",
              " ('O', 'G'): 1,\n",
              " ('YB', 'U'): 1,\n",
              " ('U', 'G'): 1,\n",
              " ('I', '</w>'): 2,\n",
              " ('l', 'o'): 1,\n",
              " ('o', 'v'): 1,\n",
              " ('v', 'e'): 1,\n",
              " ('e', '</w>'): 2,\n",
              " ('l', 'i'): 1,\n",
              " ('i', 'k'): 1,\n",
              " ('k', 'e'): 1,\n",
              " ('w', 'h'): 1,\n",
              " ('h', 'a'): 1,\n",
              " ('a', 't'): 1,\n",
              " ('t', '</w>'): 1,\n",
              " ('i', 's'): 1,\n",
              " ('s', '</w>'): 1}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "pairs = 페어등장횟수(vocab)\n",
        "pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Qg5KZzr78E"
      },
      "source": [
        "다음으로, `페어합치기` 함수를 구현해봅시다. `페어합치기` 함수는 `페어등장횟수` 함수로 계산된 페어 빈도수를 바탕으로, BPE 로직에 맞게 vocab을 업데이트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NHRf7TB41Rj3"
      },
      "outputs": [],
      "source": [
        "def 페어합치기(pairs, vocab):\n",
        "  if not pairs:\n",
        "        return vocab\n",
        "  most_frequent_pair = max(pairs, key=pairs.get)\n",
        "    \n",
        "  new_vocab = {}\n",
        "  bigram = ' '.join(most_frequent_pair)\n",
        "  replacement = ''.join(most_frequent_pair)\n",
        "  \n",
        "  for word, freq in vocab.items():\n",
        "      # 가장 자주 등장한 페어를 하나의 글자로 합침\n",
        "      new_word = word.replace(bigram, replacement)\n",
        "      new_vocab[new_word] = freq \n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "\n",
        "  return new_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C6d--MZtaPM"
      },
      "source": [
        "작성한 `페어합치기` 함수에 우리의 예시를 넣으면 다음과 같은 결과를 반환합니다.\n",
        "\n",
        "```\n",
        "{'YB A G T A </w>': 2,\n",
        " 'YB E C T A </w>': 1,\n",
        " 'YB I G T A </w>': 3,\n",
        " 'YB O G T A </w>': 1,\n",
        " 'YB U G T A </w>': 1,\n",
        " 'I </w>': 2,\n",
        " 'l o v e </w>': 1,\n",
        " 'l i k e </w>': 1,\n",
        " 'w h a t </w>': 1,\n",
        " 'i s </w>': 1}\n",
        " ```\n",
        "\n",
        " 아래 코드를 실행해 결과값이 같은지 확인해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BETYArqds9P8",
        "outputId": "21365658-190a-4602-949e-b98e569ce4bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'YB A G TA </w>': 2,\n",
              " 'YB E C TA </w>': 1,\n",
              " 'YB I G TA </w>': 3,\n",
              " 'YB O G TA </w>': 1,\n",
              " 'YB U G TA </w>': 1,\n",
              " 'I </w>': 2,\n",
              " 'l o v e </w>': 1,\n",
              " 'l i k e </w>': 1,\n",
              " 'w h a t </w>': 1,\n",
              " 'i s </w>': 1}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 다음 코드를 실행한 결과값을 위의 결과와 비교해서 함수를 잘 작성했는지 확인해보세요!\n",
        "vocab = 페어합치기(pairs,vocab)\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_oFDna61v2A"
      },
      "source": [
        "## 2. BPE Tokenizer Train (30점)\n",
        "\n",
        "이제 BPE 트레이닝을 위한 각 파트 구현을 완료했습니다! **각각의 함수를 합쳐서 BPE Tokenizer의 학습을 수행하는 `학습` 함수를 작성해주세요. (15점)**\n",
        "\n",
        " - 1번 파트에서 작성한 함수들은 한번 vocab을 업데이트하는 데 필요한 함수들입니다.\n",
        " - `n_iter`인자(int type)를 받아, 입력받은 횟수만큼 vocab을 업데이트하도록 함수를 작성해주세요.\n",
        "\n",
        "1번 파트에서 모든 함수를 잘 작성했다면, 트레이닝 함수는 간단히 작성하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OIdnzNDDoP1j"
      },
      "outputs": [],
      "source": [
        "def 학습(corpus, n_iter):\n",
        "  ##### TODO #####\n",
        "  # 함수 안을 채워주세요 #\n",
        "  # 이미 작성된 코드는 지우시면 안됩니다! #\n",
        "  vocab = 사전만들기(corpus)\n",
        "\n",
        "  for _ in range(n_iter):\n",
        "      pairs = 페어등장횟수(vocab)\n",
        "      if not pairs:\n",
        "          break\n",
        "      vocab = 페어합치기(pairs, vocab)\n",
        "\n",
        "  tokens = 토큰등장횟수(vocab)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7-6x84hxhpM"
      },
      "source": [
        "학습 함수에 sample corpus를 넣고, `n_iter`을 10으로 정한 후 시행한 결과는 다음과 같습니다.\n",
        "\n",
        "```\n",
        "{'YBAGTA</w>': 2,\n",
        " 'YB': 3,\n",
        " 'E': 1,\n",
        " 'C': 1,\n",
        " 'TA</w>': 1,\n",
        " 'YBIGTA</w>': 3,\n",
        " 'O': 1,\n",
        " 'GTA</w>': 2,\n",
        " 'U': 1,\n",
        " 'I</w>': 2,\n",
        " 'l': 2,\n",
        " 'o': 1,\n",
        " 'v': 1,\n",
        " 'e</w>': 2,\n",
        " 'i': 2,\n",
        " 'k': 1,\n",
        " 'w': 1,\n",
        " 'h': 1,\n",
        " 'a': 1,\n",
        " 't': 1,\n",
        " '</w>': 2,\n",
        " 's': 1}\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk1iZqkJv1YU",
        "outputId": "725ca289-d10a-4388-bbc4-3208d8363438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'YBAGTA</w>': 2,\n",
              " 'YB': 3,\n",
              " 'E': 1,\n",
              " 'C': 1,\n",
              " 'TA</w>': 1,\n",
              " 'YBIGTA</w>': 3,\n",
              " 'O': 1,\n",
              " 'GTA</w>': 2,\n",
              " 'U': 1,\n",
              " 'I</w>': 2,\n",
              " 'l': 2,\n",
              " 'o': 1,\n",
              " 'v': 1,\n",
              " 'e</w>': 2,\n",
              " 'i': 2,\n",
              " 'k': 1,\n",
              " 'w': 1,\n",
              " 'h': 1,\n",
              " 'a': 1,\n",
              " 't': 1,\n",
              " '</w>': 2,\n",
              " 's': 1}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = 학습(sample_corpus,n_iter=10)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hQsGGeet65Q"
      },
      "source": [
        "**또, 직접 학습을 시켜봅시다. (15점)**\n",
        "\n",
        "\n",
        "학습용 corpus는 각자 자유롭게 정해주세요. (ex. 노래 가사 등등...)\n",
        "\n",
        "- 단, 빈도수 기반의 BPE 토크나이저의 효과적인 학습을 위해서, 학습 corpus는 다음 조건을 만족하면 더 좋겠죠?\n",
        "\n",
        "  - 같은 단어가 여러개 들어가있고 적당히 긴 텍스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "O1cnQyJs1mVm"
      },
      "outputs": [],
      "source": [
        "##### TODO #####\n",
        "# 리스트 안에 train corpus를 자유롭게 채워주세요 #\n",
        "\n",
        "train_corpus = ['감사합니다 정통학교 학생대대 상병 백준호입니다.',\n",
        "              '무엇을 도와드릴까요?',\n",
        "              '잘못 들었습니다?',\n",
        "              '머스터드일까요?',\n",
        "              '아 네 필승',\n",
        "              '네? 아 네네'\n",
        "\n",
        "              ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bnJ9ynBfx5Nl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'감사합니다</w>': 1, '정': 1, '통': 1, '학': 2, '교': 1, '</w>': 7, '생': 1, '대': 2, '상': 1, '병': 1, '백': 1, '준': 1, '호': 1, '입': 1, '니다': 2, '.': 1, '무': 1, '엇': 1, '을': 1, '도': 1, '와': 1, '드': 2, '릴': 1, '까요?</w>': 2, '잘': 1, '못': 1, '들': 1, '었': 1, '습': 1, '?</w>': 2, '머': 1, '스': 1, '터': 1, '일': 1, '아</w>': 2, '네</w>': 2, '필': 1, '승': 1, '네': 2}\n"
          ]
        }
      ],
      "source": [
        "##### TODO #####\n",
        "# 여러분의 train_corpus를 구현한 BPE 토크나이저 학습 함수에 넣고, 결과값을 프린트하세요\n",
        "\n",
        "n_iter = 10\n",
        "tokens = 학습(train_corpus, n_iter)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-d6MqpC5i8o"
      },
      "source": [
        "## 보너스 질문 ( 추가점수 10점 )\n",
        "BPE 방식의 한계점은 무엇이고, 이러한 한계점이 왜 나타나는지 고민해보고 그 이유를 자유롭게 설명해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-axLI1jC6EcS"
      },
      "source": [
        "BPE (Byte Pair Encoding) 방식의 한계점은 언어의 문법적 구조나 의미를 반영하지 못해 같은 형태의 단어를 구분하지 못한다는 점, 희귀 단어는 충분히 병합되지 않아 제대로 표현되지 않는다는 점, 단어 길이에 상관없이 고정된 병합 횟수를 사용하여 긴 단어는 충분히 병합되지 않거나 짧은 단어는 과도하게 병합된다는 점, 형태소가 중요한 언어에서는 효과적이지 않다는 점 등이 있습니다. 이를 보완하기 위해 언어 모델 학습에 더 적합한 서브워드 분할 기법이 개발되고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DL_env_ybigta",
      "language": "python",
      "name": "dl_env_ybigta"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
